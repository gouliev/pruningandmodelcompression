{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "nN2CWjHRHF4w"
      },
      "source": [
        "#@title Copyright 2022 HCAIM.\n",
        "\n",
        "# Statement related to copyrights can be addeded here\n",
        "\n",
        "# Materials for this exercise are derived from the listed sources\n",
        "  #  Lab work created by Dr Rosario Catelli\n",
        "  #  Third Party Resources:\n",
        "    # https://www.tensorflow.org/api_docs\n",
        "    # https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/knowledge_distillation.ipynb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlWLbfkJtvu"
      },
      "source": [
        "#@title HCAIM Practical Details\n",
        "\n",
        "Practical Title =  Model Compression#@param\n",
        "Module = C #@param [\"A\", \"B\", \"C\", \"D\"] {type:\"raw\"}\n",
        "Focus = Future AI/Learning #@param {type:\"raw\"}\n",
        "Topic =  Knowledge Distillation#@param\n",
        "Solution_Available =  No#@param [\"Yes\", \"No\", \"NA\"] {type:\"raw\", allow-input: true}\n",
        "Duration_in_minutes = 150 #@param {type:\"slider\", min:120, max:180, step:10}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXWoPIezkzgI"
      },
      "source": [
        "## Learning Outcomes:\n",
        "\n",
        "  * Understand how to implement techniques of model compression\n",
        "  * Grasp pros and cons of knowledge distillation\n",
        "  * Becoming familiar with high-level frameworks\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZAE3blfi5h"
      },
      "source": [
        "## Lecturer Notes\n",
        "\n",
        "  * Instruction on **Quiz**: answer in the empty text cell below each quiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJJOPa6EzcH"
      },
      "source": [
        "## Instructions/Advice to students:\n",
        "\n",
        "  * This is an individual work\n",
        "  * Complete the listed taks with in the allocated time\n",
        "  * Submit all documentation related to practical on Moodle\n",
        "  * First complete the easy tasks\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_g3Hsfkzzb"
      },
      "source": [
        "## **Knowledge Distillation (KD)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Allocated_time_in_minutes = 60 #@param {type:\"slider\", min:0, max:60, step:5}"
      ],
      "metadata": {
        "id": "qqOoIX9SK-Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we will see how to build a custom `Distiller()` class and apply KD with Keras/TensorFlow. In detail:\n",
        "\n",
        "1. Dataflow and models definition\n",
        "2. Training models\n",
        "3. Distillation\n",
        "4. Comparison of the models"
      ],
      "metadata": {
        "id": "c0if1pJ8LqgE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFg7yuvOMbqk"
      },
      "source": [
        "## **Task 1 - Dataflow and models definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used for training the teacher and distilling the teacher is\n",
        "[MNIST](https://keras.io/api/datasets/mnist/), a handwritten digits dataset.\n",
        "\n",
        "Please note the procedure would be equivalent for any other dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/) (with a suitable choice of models)."
      ],
      "metadata": {
        "id": "IYs4QHFhMbqq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_q0d78zMbqq"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_btvdVyZY6rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the train and test dataset.\n",
        "# batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1: this helps us to make the network more \"stable\" during learning.\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
      ],
      "metadata": {
        "id": "LfGDR9hKY6vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, we create a teacher model and a smaller student model. Both models are\n",
        "convolutional neural networks and created using `Sequential()`,\n",
        "but could be any Keras model."
      ],
      "metadata": {
        "id": "jJM_ZSgxZGrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the teacher\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)), # Input() is used to instantiate a Keras tensor.\n",
        "                                        # A Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.\n",
        "                                        # For instance, if a, b and c are Keras tensors, it becomes possible to do: model = Model(input=[a, b], output=c)\n",
        "                                        # shape: A shape tuple (integers), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be None; 'None' elements represent dimensions where the shape is not known.\n",
        "\n",
        "        keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"), # Conv2D is a 2D convolution layer (e.g. spatial convolution over images).\n",
        "                                                                                              # This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
        "                                                                                              # filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "                                                                                              # kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "                                                                                              # strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "                                                                                              # padding: one of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding with zeros evenly to the left/right or up/down of the input. When padding=\"same\" and strides=1, the output has the same size as the input.\n",
        "\n",
        "        keras.layers.LeakyReLU(alpha=0.2),  # Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active:\n",
        "                                            # f(x) = alpha * x    if x < 0\n",
        "                                            # f(x) = x            if x >= 0\n",
        "\n",
        "        keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"), # Max pooling operation for 2D spatial data. Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.\n",
        "                                                                                  # The resulting output, when using the \"valid\" padding option, has a spatial shape (number of rows or columns) of: output_shape = math.floor((input_shape - pool_size) / strides) + 1 (when input_shape >= pool_size)\n",
        "                                                                                  # The resulting output shape when using the \"same\" padding option is: output_shape = math.floor((input_shape - 1) / strides) + 1\n",
        "                                                                                  # pool_size: Integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n",
        "                                                                                  # strides: Integer, tuple of 2 integers, or None. Strides values. Specifies how far the pooling window moves for each pooling step. If None, it will default to pool_size.\n",
        "                                                                                  # padding: One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n",
        "\n",
        "        keras.layers.Conv2D(filters=512, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"),\n",
        "\n",
        "        keras.layers.Flatten(), # Flattens the input. Does not affect the batch size.\n",
        "\n",
        "        keras.layers.Dense(10), # Just your regular densely-connected NN layer.\n",
        "                                # Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense.\n",
        "                                # Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        keras.layers.LeakyReLU(alpha=0.2),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "        keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n",
        "\n",
        "# Clone student for later comparison\n",
        "student_scratch = keras.models.clone_model(student)"
      ],
      "metadata": {
        "id": "14MFPU8jZLUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher.summary()"
      ],
      "metadata": {
        "id": "ku3wtJT7uscD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student.summary()"
      ],
      "metadata": {
        "id": "wbwvjN0MuuKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz**\n",
        "\n",
        "* What is the main difference between what we have called teacher and what we have called student?\n",
        "\n",
        "* Is it fair to take this difference as a watershed between the model concepts of *teacher* and *student*?\n",
        "\n",
        "* Why yes? Why not?"
      ],
      "metadata": {
        "id": "yd3gkJ4pZQrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answers here*"
      ],
      "metadata": {
        "id": "YqQX3GTyZWXr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJtXvVvKZl5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2 - Training models**"
      ],
      "metadata": {
        "id": "-2ihTg3RZcDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In KD we assume that the teacher is trained and fixed. Thus, we start\n",
        "by training the teacher model on the training set in the usual way."
      ],
      "metadata": {
        "id": "MuwoF211ZkFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),  # Optimizer that implements the Adam algorithm.\n",
        "                                        # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
        "\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Computes the crossentropy loss between the labels and predictions.\n",
        "                                                                        # Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers.\n",
        "                                                                        # If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss.\n",
        "                                                                        # There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true.\n",
        "\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],  # Calculates how often predictions match integer labels.\n",
        "                                                          # You can provide logits of classes as y_pred, since argmax of logits and probabilities are same.\n",
        ")\n",
        "\n",
        "# Teacher trained on data\n",
        "teacher_history = teacher.fit(x=x_train, y=y_train, validation_split=0.1, epochs=5)"
      ],
      "metadata": {
        "id": "PC7Ycd2cZnlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we figure out what is the measure of the gain given by KD?\n",
        "\n",
        "Comparing the performance between the student model trained from scratch and the analog that we will obtain from KD."
      ],
      "metadata": {
        "id": "vDq6c0YbZw3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_scratch.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Student trained from scratch.\n",
        "student_scratch_history = student_scratch.fit(x=x_train, y=y_train, validation_split=0.1, epochs=5)"
      ],
      "metadata": {
        "id": "urv7yePYZzOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 3 - Distillation**"
      ],
      "metadata": {
        "id": "s8z-9p4laHY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the KD process, knowledge is transferred through the minimization of a pair of loss functions. In particular, the goal is to match the *softened* logits of the teacher and the ground-truth labels: this is why we speak of soft loss and hard loss (the standard loss) respectively.\n",
        "\n",
        "In detail, we will speak of soft loss when the logits are softened by applying a *temperature* scaling function in the softmax, effectively smoothing out the probability distribution and revealing inter-class relationships learned by the teacher.\n",
        "\n",
        "The custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step` and `compile()`: what do these methods do?\n",
        "\n",
        "*   `train_step` -> This is the logic for one training step. This method can be overridden to support custom training logic. This method should contain the mathematical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates.\n",
        "\n",
        "*   `test_step` -> This is the logic for one evaluation step. This method can be overridden to support custom evaluation logic. This function should contain the mathematical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates.\n",
        "\n",
        "*   `compile()` -> Configures the model for training through important hyper-paramaters such as optimizer, loss, metrics and so on.\n",
        "\n",
        "In order to use the distiller, we need:\n",
        "\n",
        "- a trained teacher model and a student model to train;\n",
        "- a hard loss function: it is a student loss function on the difference between student predictions and ground-truth;\n",
        "- a soft loss function: it is a distillation loss function, along with a `temperature`, on the difference between the soft student predictions and the soft teacher labels;\n",
        "- an `alpha` factor to weight the student and distillation loss;\n",
        "- an optimizer for the student and metrics to evaluate performance.\n",
        "\n",
        "In the modified `train_step` method it is needed to:\n",
        "\n",
        "*   perform a forward pass of both teacher and student;\n",
        "*   calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and `1 - alpha`, respectively;\n",
        "*   perform the backward pass: only the student weights are updated, and therefore we only calculate the gradients for the student weights.\n",
        "\n",
        "In the modified `test_step` method, we evaluate the student model on the provided dataset."
      ],
      "metadata": {
        "id": "0ZDE1bSnaPIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):   # init method or constructor: Distiller instances will need both student and teacher parameters\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Configure the distiller.\n",
        "        Args:\n",
        "            optimizer:            Keras optimizer for the student weights\n",
        "            metrics:              Keras metrics for evaluation\n",
        "            student_loss_fn:      Loss function of difference between student predictions and ground-truth (hard loss)\n",
        "            distillation_loss_fn: Loss function of difference between soft student predictions and soft teacher predictions (soft loss)\n",
        "            alpha:                Weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature:          Temperature for softening probability distributions. Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)   # training=False == inference mode\n",
        "\n",
        "        with tf.GradientTape() as tape:                         # tf.GradientTape(): Record operations for automatic differentiation.\n",
        "\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)  # training=True == training mode\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Calculate gradients with respect to every trainable variable\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance mapping metric names to current value\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss})\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance mapping metric names to current value.\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results"
      ],
      "metadata": {
        "id": "zfUV_ONEaRtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already trained the teacher model, and we only need to initialize a\n",
        "`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n",
        "hyperparameters and optimizer, and distill knowledge from the teacher to the student."
      ],
      "metadata": {
        "id": "cLFqaMsEaXNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and compile distiller\n",
        "student_distiller = Distiller(student=student, teacher=teacher)\n",
        "student_distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(), # Computes Kullback-Leibler divergence loss between y_true and y_pred.\n",
        "                                                      # loss = y_true * log(y_true / y_pred)\n",
        "    alpha=0.1,\n",
        "    temperature=10,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "student_distilled_history = student_distiller.fit(x=x_train, y=y_train, validation_split=0.1, epochs=3)"
      ],
      "metadata": {
        "id": "KuY0UssBaaQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz**\n",
        "\n",
        "What happens changing the temperature?"
      ],
      "metadata": {
        "id": "o2d06Jc8aeDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "p7i1T8cbafrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 4 - Comparison of the models**"
      ],
      "metadata": {
        "id": "32mLo2UEajve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the performance of the models on the test set. Remember that:\n",
        "\n",
        "*   Teacher was trained for 5 epochs\n",
        "*   Student was trained from scratch for 5 epochs\n",
        "*   Student was distilled by the teacher for 3 epochs"
      ],
      "metadata": {
        "id": "3sd66bBIan2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz**\n",
        "\n",
        "*   What ranking do you expect?\n",
        "*   How would you change things?"
      ],
      "metadata": {
        "id": "KL1LXZ_LarPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answers here*"
      ],
      "metadata": {
        "id": "WQP7yAOdarYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_loss, teacher_accuracy = teacher.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"\\nTeacher\\n- Loss: {}\\n- Accuracy: {}\".format(round(teacher_loss, 5), round(teacher_accuracy, 5)))"
      ],
      "metadata": {
        "id": "KSW7Glutav8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_scratch_loss, student_scratch_accuracy = student_scratch.evaluate(x_test, y_test)\n",
        "print(\"\\nStudent trained from scratch\\n- Loss: {}\\n- Accuracy: {}\".format(round(student_scratch_loss, 5), round(student_scratch_accuracy, 5)))"
      ],
      "metadata": {
        "id": "jFhaxAfPawSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_distilled_accuracy, student_distilled_loss = student_distiller.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"\\nStudent trained through KD\\n- Loss: {}\\n- Accuracy: {}\".format(round(student_distilled_loss, 5), round(student_distilled_accuracy, 5)))"
      ],
      "metadata": {
        "id": "SJ_UXzh4awUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Independent Study Materials\n",
        "* Paper *Distilling the Knowledge in a Neural Network* at https://arxiv.org/pdf/1503.02531.pdf by Hinton et al.\n"
      ],
      "metadata": {
        "id": "VaIwzbn3NWPD"
      }
    }
  ]
}