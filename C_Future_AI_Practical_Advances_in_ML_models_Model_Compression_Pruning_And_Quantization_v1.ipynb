{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN2CWjHRHF4w"
      },
      "source": [
        "#@title Copyright 2022 HCAIM.\n",
        "\n",
        "# Statement related to copyrights can be addeded here\n",
        "\n",
        "# Materials for this exercise are derived from the listed sources\n",
        "  #  Lab work created by Dr Rosario Catelli\n",
        "  #  Third Party Resources:\n",
        "    # https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlWLbfkJtvu"
      },
      "source": [
        "\n",
        "#@title HCAIM Practical Details\n",
        "\n",
        "Practical Title =  Model Compression#@param\n",
        "Module = C #@param [\"A\", \"B\", \"C\", \"D\"] {type:\"raw\"}\n",
        "Focus = Future AI/Learning #@param {type:\"raw\"}\n",
        "Topic =  Pruning and Quantization#@param\n",
        "Solution_Available =  No#@param [\"Yes\", \"No\", \"NA\"] {type:\"raw\", allow-input: true}\n",
        "Duration_in_minutes = 150 #@param {type:\"slider\", min:120, max:180, step:10}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXWoPIezkzgI"
      },
      "source": [
        "## Learning Outcomes:\n",
        "\n",
        "  * Understand how to implement techniques of model compression\n",
        "  * Grasp the advantages of pruning and quantization\n",
        "  * Becoming familiar with high-level frameworks\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZAE3blfi5h"
      },
      "source": [
        "## Lecturer Notes\n",
        "\n",
        "  * Instruction on **Quiz**: answer in the empty text cell below each quiz.\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJJOPa6EzcH"
      },
      "source": [
        "## Instructions/Advice to students:\n",
        "\n",
        "  * This is an individual work\n",
        "  * Complete the listed taks with in the allocated time\n",
        "  * Submit all documentation related to practical on Moodle\n",
        "  * First complete the easy tasks\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH_g3Hsfkzzb"
      },
      "source": [
        "## **Pruning and Quantization**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Allocated_time_in_minutes = 70 #@param {type:\"slider\", min:0, max:70, step:5}"
      ],
      "metadata": {
        "id": "qqOoIX9SK-Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example we will see how to structure a neural network with Keras/TensorFlow and apply magnitude-based *weight pruning*. In detail:\n",
        "\n",
        "1. Prepare the dataset, create the model, train, evaluate and save the model for later usage\n",
        "2. TensorFlow Model Optimization Toolkit: model preparation for pruning\n",
        "8. Switching from TF to TFLite\n",
        "9. Adding quantization\n",
        "10. Persistence of accuracy from TF to TFLite"
      ],
      "metadata": {
        "id": "c0if1pJ8LqgE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFg7yuvOMbqk"
      },
      "source": [
        "## **Task 1 - Prepare the dataset, create the model, train, evaluate and save the model for later usage**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used for training the teacher and distilling the teacher is\n",
        "[MNIST](https://keras.io/api/datasets/mnist/), a handwritten digits dataset.\n",
        "\n",
        "Please note the procedure would be equivalent for any other dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/) (with a suitable choice of models)."
      ],
      "metadata": {
        "id": "IYs4QHFhMbqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "hMVPrc8FsPRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_q0d78zMbqq"
      },
      "source": [
        "# For the sake of simplicity, the MNIST dataset is already available within Keras so we just need to load it properly.\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data() # The method \"load_data()\" returns two tuples of NumPy arrays: (x_train, y_train), (x_test, y_test) -> We call them (train_images, train_labels), (test_images, test_labels).\n",
        "                                                                                            # x_train: uint8 NumPy array of grayscale image data with shapes (60000, 28, 28), containing the training data. Pixel values range from 0 to 255.\n",
        "                                                                                            # y_train: uint8 NumPy array of digit labels (integers in range 0-9) with shape (60000,) for the training data.\n",
        "                                                                                            # x_test: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data. Pixel values range from 0 to 255.\n",
        "                                                                                            # y_test: uint8 NumPy array of digit labels (integers in range 0-9) with shape (10000,) for the test data.\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1: this helps us to make the network more \"stable\" during learning.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model architecture using the Sequential approach given by TensorFlow.\n",
        "\n",
        "\"Model\" groups layers into an object with training and inference features."
      ],
      "metadata": {
        "id": "LBl7DjJ1slBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ow6n3XuY6QyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),  # Layer to be used as an entry point into a Network:\n",
        "                                                  # input_shape parameter: Shape tuple (not including the batch axis), or TensorShape instance (not including the batch axis).\n",
        "\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),   # Layer that reshapes inputs into the given shape.\n",
        "                                                    # target_shape: Tuple of integers, does not include the samples dimension (batch size).\n",
        "\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'), # 2D convolution layer (e.g. spatial convolution over images): this layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs\n",
        "                                                                          # filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
        "                                                                          # kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "                                                                          # activation: Activation function to use. If you don't specify anything, no activation is applied.\n",
        "                                                                          # 'relu': With default values, this returns the standard ReLU activation: max(x, 0), the element-wise maximum of 0 and the input tensor.\n",
        "\n",
        "  keras.layers.MaxPool2D(pool_size=(2, 2)), # Max pooling operation for 2D spatial data.\n",
        "                                            # Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input.\n",
        "                                            # pool_size: Integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window. If only one integer is specified, the same window length will be used for both dimensions.\n",
        "\n",
        "  keras.layers.Flatten(), # Flattens the input. Does not affect the batch size.\n",
        "\n",
        "  keras.layers.Dense(units=10)  # Just your regular densely-connected NN layer.\n",
        "                                # Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense.\n",
        "                                # Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "])"
      ],
      "metadata": {
        "id": "aPny4mexrl8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the digit classification model.\n",
        "\n",
        "The ***compile*** method configures the model for training.\n",
        "\n",
        "The ***fit*** method trains the model for a fixed number of epochs (iterations on a dataset). About the batch_size hyper-parameters:\n",
        "* batch_size: Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches)."
      ],
      "metadata": {
        "id": "Wfa4zKTTs8cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "\n",
        "    optimizer='adam', # String (name of optimizer) or optimizer instance.\n",
        "\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Loss function. May be a string (name of loss function), or a tf.keras.losses.Loss instance.\n",
        "\n",
        "    metrics=['accuracy']  # List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance.\n",
        "\n",
        "    )"
      ],
      "metadata": {
        "id": "llgTFgGus6cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7xC6WqcouinC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit(\n",
        "\n",
        "    x=train_images, # Input data\n",
        "\n",
        "    y=train_labels, # Target data\n",
        "\n",
        "    epochs=4, # Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided (unless the steps_per_epoch flag is set to something other than None). Note that in conjunction with initial_epoch, epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n",
        "\n",
        "    validation_split=0.1, # Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it,\n",
        "                          # and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling.\n",
        ")"
      ],
      "metadata": {
        "id": "1Mewsmn_6thx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ***evaluate*** method returns the loss value & metrics values for the model in test mode.\n",
        "\n",
        "Computation is done in batches. About the batch_size hyper-parameter:\n",
        "\n",
        "\n",
        "* batch_size: Integer or None. Number of samples per batch of computation. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches).\n"
      ],
      "metadata": {
        "id": "Fux9eXClvK2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile"
      ],
      "metadata": {
        "id": "xvsnb1nA4-Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_model_accuracy = model.evaluate(x=test_images, y=test_labels)  # The loss value is thrown away, while x are input data and y are target data.\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "\n",
        "_, keras_file = tempfile.mkstemp('.h5') # tempfile.mkstemp return a tuple containing an OS-level handle to (1) an open file (thrown away in this case) and (2) the absolute pathname of that file.\n",
        "tf.keras.models.save_model(model=model, filepath=keras_file, include_optimizer=False)  # Saves a model as a TensorFlow SavedModel or HDF5 file:\n",
        "# The SavedModel and HDF5 file contains:\n",
        "# - the model's configuration (topology)\n",
        "# - the model's weights\n",
        "# - the model's optimizer's state (if any)\n",
        "# Thus models can be reinstantiated in the exact same state, without any of the code used for model definition or training.\n",
        "\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "metadata": {
        "id": "PuHhOh-SvBaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2 - TensorFlow Model Optimization Toolkit: model preparation for pruning**"
      ],
      "metadata": {
        "id": "oMn-gnhGw_in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorFlow Model Optimization Toolkit is a suite of tools for optimizing ML models for deployment and execution. Among many uses, the toolkit supports techniques used to:\n",
        "* Reduce latency and inference cost for cloud and edge devices (e.g. mobile, IoT).\n",
        "* Deploy models to edge devices with restrictions on processing, memory, power-consumption, network usage, and model storage space.\n",
        "* Enable execution on and optimize for existing hardware or new special purpose accelerators."
      ],
      "metadata": {
        "id": "wtCqjkOpxHkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "vXPpp_KkxMXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "WsG90ETp7PIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will apply pruning to the whole model and see this in the model summary.\n",
        "\n",
        "In this example, you start the model with 50% sparsity (50% zeros in weights)\n",
        "and end with 80% sparsity."
      ],
      "metadata": {
        "id": "MwKL-lLjxdF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "30yQc2lZ7Ria"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude  # Modify a tf.keras layer or model to be pruned during training.\n",
        "\n",
        "# This function wraps a tf.keras model or layer with pruning functionality which sparsifies the layer's weights during training.\n",
        "# For example, using this with 50% sparsity will ensure that 50% of the layer's weights are zero.\n",
        "\n",
        "# The function accepts either a single keras layer (subclass of tf.keras.layers.Layer), list of keras layers or a Sequential or Functional tf.keras model and handles them appropriately.\n",
        "# If it encounters a layer it does not know how to handle, it will throw an error. While pruning an entire model, even a single unknown layer would lead to an error.\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1  # 10% of training set will be used for validation set.\n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split) # This is 90% of training set (i.e. 54000)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs # This will be used as stop point for pruning\n",
        "\n",
        "# Define model for pruning using PolynomialDecay: pruning will be schedule with a PolynomialDecay function.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, # Sparsity (%) at which pruning begins.\n",
        "                                                               final_sparsity=0.80,   # Sparsity (%) at which pruning ends.\n",
        "                                                               begin_step=0,          # Step at which to begin pruning.\n",
        "                                                               end_step=end_step)     # Step at which to end pruning.\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)  # ** unpacks keyword arguments\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile (equal to model.compile)\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "HaWOcPZHxnK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz**\n",
        "\n",
        "Look at the model summary: parameters are doubled respect to model.summary(). Why?"
      ],
      "metadata": {
        "id": "5CiFSFxnx54f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Your answer here*"
      ],
      "metadata": {
        "id": "XR06IoDeyRN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tune with pruning for two epochs.\n",
        "\n",
        "`tfmot.sparsity.keras.UpdatePruningStep` is required during training, and `tfmot.sparsity.keras.PruningSummaries` provides logs for tracking progress and debugging."
      ],
      "metadata": {
        "id": "m3Y-fw9hy6Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = tempfile.mkdtemp() # tempfile.mkdtemp() returns the absolute pathname of a new temporary directory.\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(), # Keras callback which updates pruning wrappers with the optimizer step. This callback must be used when training a model which needs to be pruned. Not doing so will throw an error.\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),  # A Keras callback for adding pruning summaries to tensorboard. Logs the sparsity(%) and threshold at a given iteration step.\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_images, train_labels, # Input data, Target data\n",
        "                  batch_size=batch_size,  # Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches).\n",
        "                  epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)  # List of keras.callbacks.Callback instances. List of callbacks to apply during training."
      ],
      "metadata": {
        "id": "gnUUkNojy9-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, there is minimal loss in test accuracy after pruning, compared to the baseline."
      ],
      "metadata": {
        "id": "qRj7oBl4zGXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(x=test_images, y=test_labels)  # Loss value is thrown away\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "id": "RAN7_8NJzGyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 3 - Switching from TF to TFLite: adding quantization**"
      ],
      "metadata": {
        "id": "JOPwTo0hzeIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both `tfmot.sparsity.keras.strip_pruning` and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression\n",
        "benefits of pruning.\n",
        "\n",
        "*   `strip_pruning` is necessary since it removes every tf.Variable that pruning only needs during training, which would otherwise add to model size during inference\n",
        "*   Applying a standard compression algorithm is necessary since the serialized weight matrices are the same size as they were before pruning. However, pruning makes most of the weights zeros, which is\n",
        "added redundancy that algorithms can utilize to further compress the model.\n",
        "\n",
        "First, create a compressible model for TensorFlow."
      ],
      "metadata": {
        "id": "R7ocHmkyz8mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)  # Strip pruning wrappers from the model. Once a model has been pruned to required sparsity, this method can be used to restore the original model with the sparse weights.\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')  # # tempfile.mkstemp return a tuple containing an OS-level handle to an open file (thrown away in this case) and the absolute pathname of that file.\n",
        "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)  # Saves a model as a TensorFlow SavedModel or HDF5 file:\n",
        "# The SavedModel and HDF5 file contains:\n",
        "# - the model's configuration (topology)\n",
        "# - the model's weights\n",
        "# - the model's optimizer's state (if any)\n",
        "# Thus models can be reinstantiated in the exact same state, without any of the code used for model definition or training.\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ],
      "metadata": {
        "id": "osoumwXM0BHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, create a compressible model for TFLite."
      ],
      "metadata": {
        "id": "vEPWqmOm0Era"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)  # Converts a TensorFlow model into TensorFlow Lite model.\n",
        "                                                                        # from_keras_model: creates a TFLiteConverter object from a Keras model.\n",
        "pruned_tflite_model = converter.convert() # Converts a TensorFlow GraphDef based on instance variables: the converted data in serialized format.\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite') # Already seen!\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ],
      "metadata": {
        "id": "8mhJP2dC0Gs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a helper function to actually compress the models via gzip and measure the zipped size."
      ],
      "metadata": {
        "id": "3iKEX0P30KAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "46GfNwhK0L2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare and see that the models are smaller and smaller."
      ],
      "metadata": {
        "id": "GdWvR8Mk0Nvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ],
      "metadata": {
        "id": "chv1nmu81gS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can apply post-training quantization to the pruned model for additional benefits."
      ],
      "metadata": {
        "id": "pYBaNLxO1_BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)  # Converts a TensorFlow model into TensorFlow Lite model.\n",
        "                                                                        # from_keras_model: creates a TFLiteConverter object from a Keras model.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enum defining the optimizations to apply when generating a tflite model.\n",
        "                                                      # DEFAULT Default optimization strategy that quantizes model weights. Enhanced optimizations are gained\n",
        "                                                      # by providing a representative dataset that quantizes biases and activations as well.\n",
        "                                                      # Converter will do its best to reduce size and latency, while minimizing the loss in accuracy.\n",
        "quantized_and_pruned_tflite_model = converter.convert() # Converts a TensorFlow GraphDef based on instance variables: the converted data in serialized format.\n",
        "\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite') # Already seen\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "metadata": {
        "id": "gRLfj9Ft1-NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, define a helper function to evaluate the TF Lite model on the test dataset."
      ],
      "metadata": {
        "id": "xNgDvRSE2YYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"] # Gets model input tensor details.\n",
        "  # Returns a list in which each item is a dictionary with details about an input tensor.\n",
        "  # Each dictionary contains the following fields that describe the tensor:\n",
        "  # name: The tensor name.\n",
        "  # index: The tensor index in the interpreter.\n",
        "  # shape: The shape of the tensor.\n",
        "  # and others...\n",
        "\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"] # Gets model output tensor details.\n",
        "  # Returns a list in which each item is a dictionary with details about an output tensor.\n",
        "  # The dictionary contains the same fields as described for get_input_details().\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)  # Expand the shape of an array.\n",
        "                                                                        # Insert a new axis that will appear at the axis position in the expanded array shape.\n",
        "                                                                        # e.g. if test_image.shape = (28,) then np.expand_dims(test_image, axis=0) will result in test_image.shape = (1, 28)\n",
        "\n",
        "    interpreter.set_tensor(input_index, test_image) # Sets the value of the input tensor.\n",
        "                                                    # Note this copies data in value.\n",
        "\n",
        "    # Run inference. Invoke the interpreter.\n",
        "    interpreter.invoke()  # Be sure to set the input sizes, allocate tensors and fill values before calling this.\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index) # Returns a function that can return a new numpy array pointing to the internal TFLite tensor state at any point.\n",
        "    digit = np.argmax(output()[0])  # Returns the indices of the maximum values along an axis.\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "UAzodPdi2aaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the evaluation:"
      ],
      "metadata": {
        "id": "6oPPb3U62eEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)  # Interpreter interface for running TensorFlow Lite models.\n",
        "\n",
        "interpreter.allocate_tensors()  # It allocates memory.\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter) # It passes the interpreter of the TFLite model for evaluation to the helper function created before.\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "id": "zJX5LXzD2geM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in accuracy between the pruned model in TensorFlow and the pruned and quantized model in TensorFlow Lite is minimal!"
      ],
      "metadata": {
        "id": "toXcJSrC2cNJ"
      }
    }
  ]
}